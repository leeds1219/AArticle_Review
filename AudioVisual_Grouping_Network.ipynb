{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMIJ55yyPbrVTHXloE14x6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeds1219/Article_Review/blob/main/AudioVisual_Grouping_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Audio-Visual Grouping Network for Sound Localization from Mixtures**\n",
        "Shentong MO, Yapeng Tian\n"
      ],
      "metadata": {
        "id": "yYaJk_nSuZRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?id=1tPSL_zpnv99tELSvOgUkbmE69Xr6uvNF\">\n"
      ],
      "metadata": {
        "id": "Y2MwxHeavSm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Single-source Localization & Audio-Visual Encoder"
      ],
      "metadata": {
        "id": "IW8b845pXFZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "During the training, we do not have\n",
        "bounding boxes and mask-level annotations. Therefore, we\n",
        "can only use the video-level label for the mixture spectrogram and image to perform **weakly-supervised learning**"
      ],
      "metadata": {
        "id": "LRwD4tXv6Laz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L_{baseline} = -\\frac{1}{B}\\Sigma_{b=1}^{B}log\\frac{exp(\\frac{1}{\\tau}sim(F_{b}^{a},F_{b}^{v}))}{\\Sigma_{m=1}^{B}exp(\\frac{1}{τ}sim(F_{b}^{a},F_{m}^{v}))}$"
      ],
      "metadata": {
        "id": "4V1-yDL2XJVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple-Instance Learning (MIL) framework deals with scenarios where the labels for training data are associated with sets.\n",
        "\n",
        "**Multiple-instance contrastive objective** is a technique used within this framework to train models effectively.\n",
        "\n",
        "\n",
        "Positive and Negative Pairs : positive pair of similar and negative pair of disimilar\n",
        "\n",
        "Contrastive Loss : Define a contrastive loss function that encourages the model to make the representations of positive pairs similar and those of negative pairs dissimilar.\n",
        "This loss function penalizes the model when the representations of positive pairs are not close enough and when the representations of negative pairs are too close\n",
        "\n",
        "Training Objective : The objective is to learn representations of bags in such a way that bags belonging to the same class are closer in the learned embedding space compared to bags from different classes.\n",
        "\n",
        "Learning Representations : The model learns to create embeddings/representations for bags that capture the bag-level characteristics important for classification.\n",
        "These representations are used to make predictions on new unseen bags.\n"
      ],
      "metadata": {
        "id": "LbF1BRhLB0XN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B is batch, sim() denotes the **max-pooled audio-visual cosine similarity** of $F^{a}$ and $F^{v}=\\{f_{v}^{p}\\}_{p=1}^{P}$ across all P spatial locations, D is dimension size and $\\tau$ is the temperature hyper-parameter"
      ],
      "metadata": {
        "id": "YWkuwc7ccIsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Max pooled cosine similarity** is a method used in natural language processing (NLP) to compute the similarity between two sets of embeddings.\n",
        "\n",
        "When you have multiple embeddings representing different elements within two sets, you can calculate cosine similarity between each pair of embeddings. The max pooled cosine similarity then takes the maximum similarity score among all the calculated pairwise similarities.\n",
        "\n",
        "For instance, suppose you have two sets of embeddings: Set A (embedding representations of elements from the first set) and Set B (embedding representations of elements from the second set).\n",
        "\n",
        "To compute max pooled cosine similarity:\n",
        "\n",
        "1. Calculate cosine similarity between each element in Set A and each element in Set B.\n",
        "\n",
        "2. For each element in Set A, find the highest cosine similarity among its similarities with elements in Set B.\n",
        "\n",
        "3. Take the maximum similarity score among all the highest similarities calculated in step 2.\n",
        "\n",
        "This approach can be useful in various NLP tasks, such as semantic similarity measurements, information retrieval, or text classification, where you want to determine the overall similarity between two sets based on their constituent elements' embeddings."
      ],
      "metadata": {
        "id": "TzEYrFZw6Fmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio-Visual Class Tokens(Transformer Layers)"
      ],
      "metadata": {
        "id": "p5l_cH9rWziK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C is the source event categories"
      ],
      "metadata": {
        "id": "OxHzaFwk3NEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate audio-visual tokens $\\{ \\hat{c}_{i}^{a} \\}_{i=1}^{C}$,\n",
        "Global audio and spatial visual features $\\hat{f}^{a}$"
      ],
      "metadata": {
        "id": "KKIrujevv1wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Align features and tokens from raw input of image and audio mixture with **self attention transformers**: $\\hat{f}^{a}$, $\\{ \\hat{c}_{i}^{a} \\}_{i=1}^{C}$ = $\\{\\phi(x_{j}^{a}, X^{a},X^{a})\\}_{j=1}^{1+C}$"
      ],
      "metadata": {
        "id": "zufXvmjSzWOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
        "$"
      ],
      "metadata": {
        "id": "IRtnkZ6g26WH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$X^{a}$ = $\\{x_{j}^{a}\\}_{j=1}^{1+C}$ = [$f^{a}$;$\\{c_{i}\\}_{i=1}^{C}$] is the entire set of input features, $x_{j}^{a}$ is the indiviual element in the set."
      ],
      "metadata": {
        "id": "5bwtUySs2pjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "${\\phi^{a}}(x_{j}^{a}, X^{a},X^{a}) = \\text{Softmax}\\left(\\frac{{x_{j}^{a}}{X^{a}}^T}{\\sqrt{D}}\\right) {X^{a}}\n",
        "$"
      ],
      "metadata": {
        "id": "Y6BHhE3G65Rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual source class probability with **Softmax and Fully connected layer**\n",
        "\n",
        "$e_{i}$ = Softmax(FC(${c^{i}}$))"
      ],
      "metadata": {
        "id": "LEUmwTQ-8Lc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class-Constraint Loss**\n",
        "\n",
        "**Cross entropy loss** : $\\Sigma_{i=1}^{C}CE(h_{i},e_{i})$"
      ],
      "metadata": {
        "id": "pvoSWuiW-Xh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$h_{i}$ is the **one hot encoding** with its target category entry i as 1."
      ],
      "metadata": {
        "id": "iw6Og7V1VS91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-hot encoding** is a technique used in machine learning and data processing to convert categorical data into a numerical format."
      ],
      "metadata": {
        "id": "CMuEAUhGj90O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio-Visual Grouping Module"
      ],
      "metadata": {
        "id": "tJ03a74e_Hli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouping blocks $g^{a}(\\cdot)$, $g^{v}(\\cdot)$"
      ],
      "metadata": {
        "id": "xx7GXj8qAKRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Similarity matrix**\n",
        "\n",
        "$A_{i}^{a}$ = Softmax($W_{q}^{a}\\hat{f}^{a}\\cdot W_{k}^{a}\\hat{c}_{i}^{a}$)\n",
        "\n",
        "$A_{p,i}^{v}$ = Softmax($W_{q}^{v}\\hat{f}_{p}^{v}\\cdot W_{k}^{v}\\hat{c}_{i}^{v}$)"
      ],
      "metadata": {
        "id": "ocQiGByAAarQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted Sum calculation with the similarity matrix to gain Categori-Aware Representations\n",
        "\n",
        "$g_{i}^{a}$ = $g^{a}(\\hat{f}^{a},\\hat{c}_{i}^{a})$ = $\\hat{c}_{i}^{a}$ + $W_{o}^{a}$$A_{i}^{a}\\frac{W_{v}^{a}\\hat{f}^{a}}{A_{i}^{a}}$\n",
        "\n",
        "$g_{i}^{v}$ = $g^{a}(\\{\\hat{f}_{p}^{v}\\}_{p=1}^{P},\\hat{c}_{i}^{v})$\n",
        "\n",
        "= $\\hat{c}_{i}^{v}$ + $W_{o}^{v}$$\\frac{\\Sigma_{p=1}^{P}A_{p,i}^{v}{W_{v}^{v}\\hat{f}_{p}^{v}}}{{\\Sigma_{p=1}^{P}A_{p,i}^{v}}}$"
      ],
      "metadata": {
        "id": "v7CtGjxG2NOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary probability** is defined getting the grouping representation as input\n",
        "\n",
        "$p_{i}^{a}$ = Sigmoid(FC($g_{i}^{a}$))\n",
        "\n",
        "$p_{i}^{v}$ = Sigmoid(FC($g_{i}^{v}$))"
      ],
      "metadata": {
        "id": "o__rLG1e2XNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By applying audio-visual source classes $\\{y_{i}\\}_{i=1}^{C}$ as the weak supervision and combining the class-constraint loss"
      ],
      "metadata": {
        "id": "261GHFGgMLWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Binary cross-entropy** for each category to handle multi-label classification problem"
      ],
      "metadata": {
        "id": "DjOsXhkvTBA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Audio-Visual Grouping Loss**\n",
        "\n",
        "$L_{group}$ = $\\Sigma_{i=1}^{C}\\{CE(h_{i},e_{i})+BCE(y_{i},p_{i}^{a})+BCE(y_{i},p_{i}^{v})\\}$\n"
      ],
      "metadata": {
        "id": "VPXsm3xd8C3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the class-constrained loss, categori-aware representations are generated for audio-visual alignment\n",
        "\n",
        "$\\{g_{i}^{a}\\}_{i=1}^{C}$\n",
        "\n",
        "$\\{g_{i}^{v}\\}_{i=1}^{C}$\n",
        "\n",
        "global audio and visual representations for N source embeddings\n",
        "\n",
        "$\\{g_{n}^{a}\\}_{n=1}^{N}$\n",
        "\n",
        "$\\{g_{n}^{v}\\}_{n=1}^{N}$\n",
        "\n",
        "are chosen from C categories according to **ground truth class(y)**"
      ],
      "metadata": {
        "id": "cqfkneAfHkSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Localization"
      ],
      "metadata": {
        "id": "QFrtu8CnxVBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Audio-visual similarity is calculated by **max pooling audio-visual cosine similarities** of the class-aware audio features $F_{b,n}^{a}$\n",
        "= $g_{n}^{a}$ and the spatial-level $F_{b,n}^{v}$ = $\\{f_{p}^{v}\\odot g_{n}^{v}\\}_{p=1}^{P}$\n",
        "\n",
        "meaning $n$th source in $b$th sample"
      ],
      "metadata": {
        "id": "gQ4YqHQHSm64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L_{loc}$ = $-\\frac{1}{BN}\\Sigma_{b=1}^{B}\\Sigma_{n=1}^{N}log\\frac{exp(\\frac{1}{τ}sim(F_{b,n}^{a},F_{b,n}^{v}))}{\\Sigma_{m=1}^{B}exp(\\frac{1}{τ}sim(F_{b,n}^{c},F_{m,n}^{v}))}$"
      ],
      "metadata": {
        "id": "rzpHBFtH8hnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representations for N source embeddings are chosen from C categories according to the corresponding GT class"
      ],
      "metadata": {
        "id": "J_UniP6wWAVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B is mini-batch size, N is sources in each batch"
      ],
      "metadata": {
        "id": "KVbawQm9UNwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "End-to-end loss $L$ = $L_{loc}+L_{group}$"
      ],
      "metadata": {
        "id": "V1DGpH1PWo77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "EJgeUCETz_oP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use audio-visual **cosine similarity map** between class-aware audio-visual representations to generate $n$the source localization map with $P$ locations.\n",
        "\n",
        "The final localization map is gerated through **bilinear interpolation** of the similarity map."
      ],
      "metadata": {
        "id": "ant6phNHZOT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bilinear interpolation** is particularly useful in RoI Align when estimating values at fractional coordinates within feature maps, providing a more accurate representation of the spatial information than methods like max pooling or average pooling."
      ],
      "metadata": {
        "id": "4uZnEMYZtvT8"
      }
    }
  ]
}