{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "yYaJk_nSuZRr",
        "IW8b845pXFZN",
        "p5l_cH9rWziK",
        "tJ03a74e_Hli",
        "QFrtu8CnxVBN",
        "EJgeUCETz_oP"
      ],
      "authorship_tag": "ABX9TyNYgdnt9UlaHyKPMFH1rrGc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeds1219/Article_Review/blob/main/AudioVisual_Grouping_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Audio-Visual Grouping Network for Sound Localization from Mixtures**\n",
        "Shentong MO, Yapeng Tian\n"
      ],
      "metadata": {
        "id": "yYaJk_nSuZRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?id=1tPSL_zpnv99tELSvOgUkbmE69Xr6uvNF\">\n"
      ],
      "metadata": {
        "id": "Y2MwxHeavSm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Single-source Localization & Audio-Visual Encoder"
      ],
      "metadata": {
        "id": "IW8b845pXFZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L_{baseline} = -\\frac{1}{B}\\Sigma_{b=1}^{B}log\\frac{exp(\\frac{1}{\\tau}sim(F_{b}^{a},F_{b}^{v}))}{\\Sigma_{m=1}^{B}exp(\\frac{1}{τ}sim(F_{b}^{a},F_{m}^{v}))}$"
      ],
      "metadata": {
        "id": "4V1-yDL2XJVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B is batch, sim() denotes the max-pooled audio-visual cosine similarity of $F^{a}$ and $F^{v}=\\{f_{v}^{p}\\}_{p=1}^{P}$ across all P spatial locations, D is dimension size and $\\tau$ is the temperature hyper-parameter"
      ],
      "metadata": {
        "id": "YWkuwc7ccIsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio-Visual Class Tokens(Transformer Layers)"
      ],
      "metadata": {
        "id": "p5l_cH9rWziK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C is the source event categories"
      ],
      "metadata": {
        "id": "OxHzaFwk3NEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggregate audio-visual tokens $\\{ \\hat{c}_{i}^{a} \\}_{i=1}^{C}$,\n",
        "Global audio and spatial visual features $\\hat{f}^{a}$"
      ],
      "metadata": {
        "id": "KKIrujevv1wQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Align features and tokens from raw input of image and audio mixture with self attention transformers: $\\hat{f}^{a}$, $\\{ \\hat{c}_{i}^{a} \\}_{i=1}^{C}$ = $\\{\\phi(x_{j}^{a}, X^{a},X^{a})\\}_{j=1}^{1+C}$"
      ],
      "metadata": {
        "id": "zufXvmjSzWOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
        "$"
      ],
      "metadata": {
        "id": "IRtnkZ6g26WH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$X^{a}$ = $\\{x_{j}^{a}\\}_{j=1}^{1+C}$ = [$f^{a}$;$\\{c_{i}\\}_{i=1}^{C}$] is the entire set of input features, $x_{j}^{a}$ is the indiviual element in the set."
      ],
      "metadata": {
        "id": "5bwtUySs2pjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "${\\phi^{a}}(x_{j}^{a}, X^{a},X^{a}) = \\text{Softmax}\\left(\\frac{{x_{j}^{a}}{X^{a}}^T}{\\sqrt{D}}\\right) {X^{a}}\n",
        "$"
      ],
      "metadata": {
        "id": "Y6BHhE3G65Rk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Individual source class probability $e_{i}$ = Softmax(FC(${c^{i}}$))"
      ],
      "metadata": {
        "id": "LEUmwTQ-8Lc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross entropy loss: $\\Sigma_{i=1}^{C}CE(h_{i},e_{i})$"
      ],
      "metadata": {
        "id": "pvoSWuiW-Xh2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$h_{i}$ is the one hot encoding with its target category entry i as 1."
      ],
      "metadata": {
        "id": "iw6Og7V1VS91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding is a technique used in machine learning and data processing to convert categorical data into a numerical format."
      ],
      "metadata": {
        "id": "CMuEAUhGj90O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio-Visual Grouping Module"
      ],
      "metadata": {
        "id": "tJ03a74e_Hli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouping blocks $g^{a}(\\cdot)$, $g^{v}(\\cdot)$"
      ],
      "metadata": {
        "id": "xx7GXj8qAKRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity matrix\n",
        "\n",
        "$A_{i}^{a}$ = Softmax($W_{q}^{a}\\hat{f}^{a}\\cdot W_{k}^{a}\\hat{c}_{i}^{a}$)\n",
        "\n",
        "$A_{p,i}^{v}$ = Softmax($W_{q}^{v}\\hat{f}_{p}^{v}\\cdot W_{k}^{v}\\hat{c}_{i}^{v}$)"
      ],
      "metadata": {
        "id": "ocQiGByAAarQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weighted Sum calculation with the similarity matrix to gain Categori-Aware Representations\n",
        "\n",
        "$g_{i}^{a}$ = $g^{a}(\\hat{f}^{a},\\hat{c}_{i}^{a})$ = $\\hat{c}_{i}^{a}$ + $W_{o}^{a}$$A_{i}^{a}\\frac{W_{v}^{a}\\hat{f}^{a}}{A_{i}^{a}}$\n",
        "\n",
        "$g_{i}^{v}$ = $g^{a}(\\{\\hat{f}_{p}^{v}\\}_{p=1}^{P},\\hat{c}_{i}^{v})$\n",
        "\n",
        "= $\\hat{c}_{i}^{v}$ + $W_{o}^{v}$$\\frac{\\Sigma_{p=1}^{P}A_{p,i}^{v}{W_{v}^{v}\\hat{f}_{p}^{v}}}{{\\Sigma_{p=1}^{P}A_{p,i}^{v}}}$"
      ],
      "metadata": {
        "id": "v7CtGjxG2NOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary probability is defined getting the grouping as input $p_{i}^{a}$ = Sigmoid(FC($g_{i}^{a}$))"
      ],
      "metadata": {
        "id": "o__rLG1e2XNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary cross-entropy for multiple audio sources in a mixture, multi-label classification problem"
      ],
      "metadata": {
        "id": "DjOsXhkvTBA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L_{group}$ = $\\Sigma_{i=1}^{C}\\{CE(h_{i},e_{i})+BCE(y_{i},p_{i}^{a})+BCE(y_{i},p_{i}^{v})\\}$\n"
      ],
      "metadata": {
        "id": "VPXsm3xd8C3z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Localization"
      ],
      "metadata": {
        "id": "QFrtu8CnxVBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Audio-visual similarity is calculated by max pooling audio-visual cosine similarities of the class-aware audio features $g_{n}^{a}$ and the spatial-level"
      ],
      "metadata": {
        "id": "gQ4YqHQHSm64"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L_{loc}$ = $-\\frac{1}{BN}\\Sigma_{b=1}^{B}\\Sigma_{n=1}^{N}log\\frac{exp(\\frac{1}{τ}sim(F_{b,n}^{a},F_{b,n}^{v}))}{\\Sigma_{m=1}^{B}exp(\\frac{1}{τ}sim(F_{b,n}^{c},F_{m,n}^{v}))}$"
      ],
      "metadata": {
        "id": "rzpHBFtH8hnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Representations for N source embeddings are chosen from C categories according to the corresponding GT class"
      ],
      "metadata": {
        "id": "J_UniP6wWAVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B is mini-batch size, N is sources in each batch"
      ],
      "metadata": {
        "id": "KVbawQm9UNwM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "End-to-end loss $L$ = $L_{loc}+L_{group}$"
      ],
      "metadata": {
        "id": "V1DGpH1PWo77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "EJgeUCETz_oP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use audio-visual cosine similarity map between class-aware audio-visual representations to genrate $n$the source localization map with $P$ locations.\n",
        "\n",
        "The final localization map is gerated through bilinear interpolation of the similarity map."
      ],
      "metadata": {
        "id": "ant6phNHZOT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bilinear interpolation is particularly useful in RoI Align when estimating values at fractional coordinates within feature maps, providing a more accurate representation of the spatial information than methods like max pooling or average pooling."
      ],
      "metadata": {
        "id": "4uZnEMYZtvT8"
      }
    }
  ]
}