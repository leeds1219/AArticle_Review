{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8PWATrDf4Cqv",
        "i4kUBVBG94mi",
        "tohObFab4P2j",
        "VMf-41QT6sHd"
      ],
      "authorship_tag": "ABX9TyM39xc4xy0ox92tbELY5EC+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeds1219/AArticle_Review/blob/main/Audio_visual_SAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sounding Object Localization**"
      ],
      "metadata": {
        "id": "xni36t1pn3gL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Finding the object**\n",
        "\n",
        "Object detection -> Bounding boxes\n",
        "\n",
        "Pixel-wise mapping -> Attention heat map\n",
        "\n",
        "SAM -> Segmentation mask"
      ],
      "metadata": {
        "id": "HkOu8Ga9n9uL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Annotation Free**\n",
        "\n",
        "Text as label -> CLIP\n",
        "\n",
        "SAM -> Segmentation mask"
      ],
      "metadata": {
        "id": "uWxX5outqdJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAM structure**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1W1aL3_witNYfkGKSja1oPzQbY5fHKSme\">\n",
        "\n"
      ],
      "metadata": {
        "id": "8PWATrDf4Cqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Image encoder: ViT\n"
      ],
      "metadata": {
        "id": "VO7h_K8snNAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt encoder: two categories\n",
        "\n",
        "1. Sparse prompts(points, boxes, texts)\n",
        "\n",
        "Points and boxes: positional encodings and learned embeddings\n",
        "\n",
        "Texts: CLIP text encoder\n",
        "\n",
        "2. Masks\n",
        "\n",
        "Masks: summed into the image embeddings with convolution\n",
        "\n",
        "if no prompts SAM generate the segmentation proposals by traversing the entire image"
      ],
      "metadata": {
        "id": "ZQiaYd-K8PPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mask decoder: light-weighted with only two transformer decoder layers\n",
        "\n",
        "prompts including points, boxes and masks\n",
        "generally only provide explicit location information of target segmentation\n",
        "\n",
        "This is not suitable for audio-visual tasks"
      ],
      "metadata": {
        "id": "EgkCK3uJ8R-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image = I, GT mask = G\n",
        "\n",
        "SAM predicts N masks = $M_{i}$\n",
        "\n",
        "calculate IoU and select the highest IoU as final prediction"
      ],
      "metadata": {
        "id": "glptY_l44PMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AV-SAM**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1dvxPchMkUTm8j-vd1b3-1arXcK6aTkRS\">\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i4kUBVBG94mi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pixel-wise\n",
        "Audio-Visual Fusion\n",
        "\n",
        "$z_{i}^{s}= v_{i}^{s} + μ(\\frac{\\theta(v_{i}^{s})\\phi(\\hat{a}_{i})^{T}}{H^{s}W^{s}}ω(v_{i}^{s}))$\n",
        "\n",
        "the operations are 1 by 1 convolution\n",
        "\n",
        "Audio-Visual Mask Decoder\n",
        "\n",
        "multi-scale feature maps $z_{i}^{s}$ are passed into the mask decoder and prompt encoder\n",
        "\n",
        "the final mask M and the pixel-level annotation Y is formulated as BCE(M,Y)"
      ],
      "metadata": {
        "id": "2ii2ZbZXnIj8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Audio-Prompted SAM**\n"
      ],
      "metadata": {
        "id": "tohObFab4P2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treating Audio embeddings as prompt tokens\n",
        "\n",
        "image embedding(254 x 64 x 64) and audio prompt tokens(1 x 254) are passed to the light-weight mask decoder\n"
      ],
      "metadata": {
        "id": "CuzljfGrnP5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAMA-AVS**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1k3f3ZjXSF5RaK3B6WlgvypdzOMeDY_Xs\">\n"
      ],
      "metadata": {
        "id": "VMf-41QT6sHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fusion -> not\n",
        "adequate to drive the AVS task\n",
        "\n",
        "Injection -> injecting the audio information\n",
        "into the image features at encoding process is costly\n",
        "\n",
        "Adapters (like AVFormer)\n",
        "\n",
        "Y is the output of i-1th transformer layer\n",
        "\n",
        "T is the output of ith adapter\n",
        "\n",
        "X is the input for the ith encoder\n",
        "\n",
        "$X^{i}=Y^{i-1}+T^{i}$\n",
        "\n",
        "Adapter design: two-layer MLPs\n",
        "\n",
        "Adjust feature dimension and perform feature extraction\n",
        "\n"
      ],
      "metadata": {
        "id": "lYlIOwa6nTRF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison\n"
      ],
      "metadata": {
        "id": "V-OSZzwxUwfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?id=1OANGPr-ZeDRHrd1GYWzkGMx5jJniwr3p\">\n",
        "\n",
        "Intersection-over-Union & F-score"
      ],
      "metadata": {
        "id": "8ruBFjDmnX0x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Audio-Visual Sound Separation**"
      ],
      "metadata": {
        "id": "GQstU5PVnAl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sound Separation**\n",
        "\n",
        "Predict a mask for the audio spectrogram"
      ],
      "metadata": {
        "id": "il1F9yT2nlsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluations**\n",
        "\n",
        "SNR(Signal-to-Noise Ratio)\n",
        "\n",
        "SDR (Signal-to-Distortion Ratio)\n",
        "\n",
        "SI-SDR(Scale-Invariant SDR)"
      ],
      "metadata": {
        "id": "-cPnEjsPsMJI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PC(Predictive Coding)Net**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1Y1rRIYXUWqSdxacM7adr8ZnuUNvwBYPq\">"
      ],
      "metadata": {
        "id": "O6FQWY35wm9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diffusion model for Sound Separation**\n",
        "<img src = \"https://drive.google.com/uc?id=1VfBNB80JAyQZOj3aiIyRPd84ksNBO_Hn\">"
      ],
      "metadata": {
        "id": "zkLX4bKNwn8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?id=1BeJofa2Ffi5uSGxcmu2p_w4auGa-g5Hk\">"
      ],
      "metadata": {
        "id": "yJ2ypKM4yRm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UNet**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1arbtm9rIdaMBzfYa9R4WgBTDBOVl6ocs\">"
      ],
      "metadata": {
        "id": "fY2NidjYtsKk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?id=1nueZanoQ74geGg9Dv9E2HyKH_hiXqJyH\">"
      ],
      "metadata": {
        "id": "GOR5Qg0nqycR"
      }
    }
  ]
}