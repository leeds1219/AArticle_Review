{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM25S9Z5j49hCj/QHbZlW+A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leeds1219/AArticle_Review/blob/main/Audio_visual_SAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAM structure**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1W1aL3_witNYfkGKSja1oPzQbY5fHKSme\">\n",
        "\n",
        "Image encoder: ViT\n",
        "\n"
      ],
      "metadata": {
        "id": "8PWATrDf4Cqv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt encoder: two categories\n",
        "\n",
        "1. Sparse prompts(points, boxes, texts)\n",
        "\n",
        "Points and boxes: positional encodings and learned embeddings\n",
        "\n",
        "Texts: CLIP text encoder\n",
        "\n",
        "2. Masks\n",
        "\n",
        "Masks: summed into the image embeddings with convolution\n",
        "\n",
        "if no prompts SAM generate the segmentation proposals by traversing the entire image"
      ],
      "metadata": {
        "id": "ZQiaYd-K8PPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mask decoder: light-weighted with only two transformer decoder layers\n",
        "\n",
        "prompts including points, boxes and masks\n",
        "generally only provide explicit location information of target segmentation\n",
        "\n",
        "This is not suitable for audio-visual tasks"
      ],
      "metadata": {
        "id": "EgkCK3uJ8R-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image = I, GT mask = G\n",
        "\n",
        "SAM predicts N masks = $M_{i}$\n",
        "\n",
        "calculate IoU and select the highest IoU as final prediction"
      ],
      "metadata": {
        "id": "glptY_l44PMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Audio-Prompted SAM**\n",
        "\n",
        "Treating Audio embeddings as prompt tokens\n",
        "\n",
        "image embedding(254 x 64 x 64) and audio prompt tokens(1 x 254) are passed to the light-weight mask decoder\n"
      ],
      "metadata": {
        "id": "tohObFab4P2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAMA-AVS**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1k3f3ZjXSF5RaK3B6WlgvypdzOMeDY_Xs\">\n",
        "\n",
        "Fusion -> not\n",
        "adequate to drive the AVS task\n",
        "\n",
        "Injection -> injecting the audio information\n",
        "into the image features at encoding process is costly\n",
        "\n",
        "Adapters (like AVFormer)\n",
        "\n"
      ],
      "metadata": {
        "id": "VMf-41QT6sHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AV-SAM**\n",
        "\n",
        "<img src = \"https://drive.google.com/uc?id=1dvxPchMkUTm8j-vd1b3-1arXcK6aTkRS\">\n",
        "\n",
        "The pixel-wise\n",
        "audio-visual fusion\n",
        "\n"
      ],
      "metadata": {
        "id": "i4kUBVBG94mi"
      }
    }
  ]
}